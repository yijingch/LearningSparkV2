{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Introduction\n",
    "\n",
    "skip this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Getting Started\n",
    "\n",
    "Spark download page: https://spark.apache.org/downloads.html\n",
    "\n",
    "Or... since the release of Apache Spark 2.2, developers who only care about learning Spark in Python (that's me) have the option of installing pyspark through `pip install pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation, Action and Lazy Evaluations\n",
    "\n",
    "- **Transformation**: a transformation transform a Spark DataFrame into a new DataFrame without altering the original data, giving it the property of immutability\n",
    "    - their results are not computed immediately, but they are recorded or remembered as a _lineage_\n",
    "    - lazy evaluation is Spark’s strategy for delaying execution until an action is invoked or data is \"touched\"\n",
    "    - e.g., `orderBy()`, `groupBy()`, `filter()`, `select()`, `join()`\n",
    "- **Action**: an action triggers the lazy evaluation of all the recorded transformations\n",
    "    - e.g., `show()`, `take()`, `count()`, `collect()`, `save()`\n",
    "    \n",
    "    \n",
    "### Narrow vs. wide transformation\n",
    "\n",
    "- **Narrow**: a single output partition can be computed from a single input partition\n",
    "    - e.g., `filter()`, `contains()`\n",
    "- **Wide**: data from other partitions is read in, combined, and written to disk\n",
    "    - e.g., `groupBy()`, `orderBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access The Spark UI\n",
    "\n",
    "1. launch `pyspark` from terminal\n",
    "2. `Spark context Web UI available at http://10.0.0.242:4040` --> open in a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Apache Spark's Structured APIs\n",
    "\n",
    "## RDD\n",
    "\n",
    "- **3 vital characteristics**: \n",
    "    - dependencies: instructs Spark how an RDD is constructed with its inputs is required\n",
    "    - partitions: provides Spark the ability to split the work to parallelize computation on partitions across executors\n",
    "    - compute function: produces an `Iterator[T]` for the data that will be stored in RDD\n",
    "\n",
    "## Python data types in Spark\n",
    "\n",
    "- basics: `ByteType`, `ShortType`, `IntegerType`, `LongType`, `FloatType`, `DoubleType`, `StringType`, `BooleanType`, `DecimalType`\n",
    "- more sophisticated: `BinaryType`, `TimestampType`, `DateType`, `ArrayType`, `MapType`, `StructType`, `StructField`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Spark SQL and DataFrames: Introduction to Built-in Data Sources\n",
    "\n",
    "- DataFrame --> SQL: `df.createOrReplaceTempView([TABLENAME])`\n",
    "- read and write data in different formats (check dbc notebook 4.2)\n",
    "     * Parquet\n",
    "     * JSON\n",
    "     * CSV\n",
    "     * Avro\n",
    "     * ORC\n",
    "     * Image\n",
    "     * Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Spark SQL and DataFrames: Interacting with External Data Sources\n",
    "\n",
    "## create UDFs\n",
    "\n",
    "- can create UDFs for both SQL and pandas\n",
    "- Null checking: \n",
    "    1. Make the UDF itself `null`-aware and do null checking inside the UDF\n",
    "    2. Use IF or CASE WHEN expressions to do the `null` check and invoke the UDF in a conditional branch\n",
    "    \n",
    "## querying with the Spark SQL Shell, Beeline, and Tableau\n",
    "\n",
    "(skipped for now)\n",
    "\n",
    "## common DataFrames and Spark SQL operations\n",
    "\n",
    "Spark SQL doc: https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "\n",
    "- `union()`: union two different DataFrames with the same schema together\n",
    "- `join()`: default inner join\n",
    "- windowing: e.g., `rank()`, `denseRank()` (interesting), `percentRank()` (p149)\n",
    "- modifications\n",
    "    - adding: `withColumn()`\n",
    "    - dropping: `drop()`\n",
    "    - renaming: `withColumnRenamed()`\n",
    "    - pivoting: `PIVOT` (p154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Spark SQL and Datasets\n",
    "\n",
    "In this chapter, we go under the hood to understand Datasets: we’ll explore working with Datasets in **Java and Scala**, how Spark manages memory to accommodate Dataset constructs as part of the high-level API, and the costs associated with using Datasets.\n",
    "\n",
    "(skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Optimizing and Tuning Spark Applications\n",
    "\n",
    "## Viewing and modifying Spark properties:\n",
    "- configuration files in your deployment’s `$SPARK_HOME` directory\n",
    "- specify Spark configurations directly in your Spark application\n",
    "    - or, on the command line when submitting the application with `spark-submit`, using the `--conf` flag\n",
    "\n",
    "```\n",
    "  spark-submit --conf spark.sql.shuffle.partitions=5 --conf\n",
    "    \"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/main-\n",
    "    scala-chapter7_2.12-1.0.jar\n",
    "```\n",
    "- through a programmatic interface via the Spark shell\n",
    "\n",
    "## Scaling Spark for Large Workloads\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
